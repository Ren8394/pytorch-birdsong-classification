{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from src.utils import SegmentWithSlidingWindow, GetSortedSpeciesCode, OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(str(Path.cwd().parent.joinpath('setting', 'config.ini')))\n",
    "\n",
    "WIN_LEN = config['Window'].getint('Length')\n",
    "HOP_LEN = WIN_LEN * (1 - config['Window'].getfloat('Overlap'))\n",
    "TARGET_SPECIES = GetSortedSpeciesCode(Path.cwd().parent.joinpath('setting', 'SPECIES.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing noise\n",
    "\n",
    "1. 取得所有未降噪音檔路徑\n",
    "2. 讀取未降噪音檔, 標準化音檔, 最後使用 [noisereduce](https://github.com/timsainb/noisereduce) 進行降噪 \n",
    "3. 輸出降噪後檔案於 \"_pwd/../data/NrAudio/_\"\n",
    "4. 刪除未降噪檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceNoise(filePaths:Path, remove:bool=True):\n",
    "  for filePath in tqdm_notebook(filePaths):\n",
    "    audio, sr = librosa.load(str(filePath), sr=None)\n",
    "    audio = librosa.util.normalize(audio.T)\n",
    "    nrAudio = nr.reduce_noise(y=audio, sr=sr, use_tqdm=True)\n",
    "    sf.write(\n",
    "      Path.cwd().parent.joinpath('data', 'NrAudio', f'{filePath.stem}.wav'),\n",
    "      data=nrAudio, samplerate=sr, subtype='PCM_24'\n",
    "    )\n",
    "    if remove:\n",
    "      Path.unlink(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ef86c9834145bb8cce09e09040f714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3aa61cd5a142a4bfeeca1f32d4ceba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/544 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rawAudioPaths = sorted(Path.cwd().parent.joinpath('data', 'Audio').glob('*.wav'))\n",
    "openAudioPaths = \\\n",
    "  sorted(Path.cwd().parent.joinpath('data', 'OpenSource').glob('*.mp3')) + \\\n",
    "  sorted(Path.cwd().parent.joinpath('data', 'OpenSource').glob('*.wav'))\n",
    "\n",
    "reduceNoise(rawAudioPaths, remove=True)   # reduce noise and delete raw files\n",
    "reduceNoise(openAudioPaths, remove=False) # reduce noise and keep raw files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Auto-encoder Classifier dataset\n",
    "\n",
    "將人工標記檔案細切，以取得乾淨、較高品質資料\n",
    "\n",
    "1. 去除非屬song-type之鳥音\n",
    "2. 將剩餘鳥音以適當演算法進行切割\n",
    "3. 切除後鳥音如大於 WIN_LEN 則進行滑移視窗處理, 如否則不處理\n",
    "4. 保留資料時長大於等於0.5秒標記"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentSignalLabel(df:pd.DataFrame):\n",
    "  records = []\n",
    "  labels = []\n",
    "  for i, x in df.iterrows():\n",
    "    records.append([x['start time'], 'L', i])\n",
    "    records.append([x['end time'], 'R', i])\n",
    "    labels.append(x['label'])\n",
    "  # Sort by time\n",
    "  records = sorted(records)\n",
    "\n",
    "  overlap = []\n",
    "  results = []\n",
    "  for j, record in enumerate(records):\n",
    "    if record[1] == 'L':\n",
    "      if overlap: # Overlap means get L but previous L' aren't finished by its R'\n",
    "        labelList = [label for k, label in enumerate(labels) if k in overlap]\n",
    "        labelList = set(labelList)  # Use set to remove identical label\n",
    "        results.append([records[j-1][0], record[0], labelList])\n",
    "        overlap.append(record[2])\n",
    "      else:\n",
    "        overlap.append(record[2])\n",
    "    else:\n",
    "      labelList = [label for k, label in enumerate(labels) if k in overlap]\n",
    "      labelList = set(labelList)\n",
    "      results.append([records[j-1][0], record[0], labelList])\n",
    "      overlap.remove(record[2])\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateClassifierDataset(filePaths:Path, outputFilename:str):\n",
    "  df = pd.DataFrame(columns=['file', 'start time', 'end time', 'label'])\n",
    "  for filePath in tqdm_notebook(filePaths):\n",
    "    # Preporcess\n",
    "    singleDF = pd.read_csv(filePath, sep='\\t', names=['start time', 'end time', 'label'])\n",
    "    singleDF['label'] = singleDF['label'].str.upper().replace(' ', '')\n",
    "    singleDF = singleDF[singleDF['label'].str.contains('-S+', regex=True, na=False)]\n",
    "    singleDF.reset_index(drop=True, inplace=True)\n",
    "    singleDF['label'] = singleDF['label'].apply(lambda x: str(x).split('-')[0])\n",
    "    \n",
    "    if singleDF.empty:\n",
    "      continue\n",
    "\n",
    "    # Segment signal interval\n",
    "    segmentedDF = pd.DataFrame(segmentSignalLabel(singleDF), columns=['start time', 'end time', 'label'])\n",
    "    source = sf.SoundFile(Path.cwd().parent.joinpath('data', 'NrAudio', f'{filePath.stem}.wav'))\n",
    "    segmentedDF['end time'] = segmentedDF['end time'].apply(lambda x: min(x, source.frames / source.samplerate))\n",
    "    segmentedDF['file'] = Path('NrAudio', f'{filePath.stem}.wav')\n",
    "    segmentedDF = segmentedDF[['file', 'start time', 'end time', 'label']]\n",
    "\n",
    "    # Sliding window\n",
    "    shortDF = segmentedDF[segmentedDF['end time'] - segmentedDF['start time'] <= WIN_LEN]   # Duration less and equal than {WIN_LEN}\n",
    "    longDF = segmentedDF[segmentedDF['end time'] - segmentedDF['start time'] > WIN_LEN]     # Duration greater than {WIN_LEN}\n",
    "    for _, x in longDF.iterrows():\n",
    "      st, et = x['start time'], x['end time'] - WIN_LEN\n",
    "      slidingWindow = []\n",
    "      while st <= et:\n",
    "        slidingWindow.append([x['file'], np.around(st, decimals=6), np.around(st + WIN_LEN, decimals=6), x['label']])\n",
    "        st += HOP_LEN\n",
    "      shortDF = pd.concat(\n",
    "        [shortDF, pd.DataFrame(slidingWindow, columns=['file', 'start time', 'end time', 'label'])],\n",
    "        ignore_index=True\n",
    "      )\n",
    "\n",
    "    # Filter\n",
    "    shortDF = shortDF[shortDF['end time'] - shortDF['start time'] > 0.5]    # Only duration > 0.5 are accept\n",
    "    shortDF.sort_values(by=['file', 'start time', 'end time'], inplace=True)\n",
    "    shortDF['label'] = shortDF['label'].apply(lambda x: sorted(x))\n",
    "\n",
    "    # Concatenate\n",
    "    df = pd.concat([df, shortDF], ignore_index=True)\n",
    "  \n",
    "  # Save results\n",
    "  df['label'] = df['label'].apply(lambda x: ','.join(x))\n",
    "  df.to_csv(Path.cwd().parent.joinpath('data', f'{outputFilename}.csv'), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5c987764b74ecdbc02b34805acc711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48305299cdc4e618889037644018fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selfLabelPaths = sorted(Path.cwd().parent.joinpath('data', 'Label').glob('*.txt'))\n",
    "openLabelPaths = sorted(Path.cwd().parent.joinpath('data', 'OpenSource').glob('*.txt'))\n",
    "\n",
    "generateClassifierDataset(selfLabelPaths, 'manual-dataset')\n",
    "generateClassifierDataset(openLabelPaths, 'opensource-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Auto-encoder dataset\n",
    "\n",
    "以所有已降噪音訊製作 auto-encoder 資料集\n",
    "\n",
    "1. 讀取音檔並取得其播放長度\n",
    "2. 以滑移視窗的方式切割出資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAEDataset(filePaths:Path):\n",
    "  ae = []\n",
    "  for filePath in tqdm_notebook(filePaths):\n",
    "    source = sf.SoundFile(filePath)\n",
    "    stList = SegmentWithSlidingWindow(\n",
    "      length=source.frames / source.samplerate,\n",
    "      windowLength=WIN_LEN, hopLength=HOP_LEN\n",
    "    )\n",
    "    for st in stList:\n",
    "      ae.append([Path('NrAudio', f'{filePath.stem}.wav'), st, st + WIN_LEN])\n",
    "  \n",
    "  df = pd.DataFrame(ae, columns=['file', 'start time', 'end time'])\n",
    "  df.to_csv(Path.cwd().parent.joinpath('data', f'ae-dataset.csv'), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9765843c25400e8207930107cd74ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrAudioPaths = sorted(Path.cwd().parent.joinpath('data', 'NrAudio').glob('*.wav'))\n",
    "generateAEDataset(nrAudioPaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating dataset for model\n",
    "\n",
    "共 3 個資料集, 第一個為自動標記資料集, 下一個為人工標記資料集, 最後是開源資料集。  \n",
    "目前將所有資料集丟入 Model 進行訓練, 唯開源資料集不進入 Model 測試\n",
    "\n",
    "1. 決定切割比例\n",
    "2. 將 label 以 onehot 模式表示\n",
    "3. 切割資料集為 train, validate, test 三群 (Note: auto-encoder 無 test)\n",
    "4. 儲存資料進暫時資料夾 \"_pwd/../data/tmp/_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierRatio = (0.8, 0.1, 0.1)\n",
    "autoencoderRatio = (0.9, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTarget(df:pd.DataFrame):\n",
    "  df['label'] = df['label'].apply(lambda x: [l for l in x.split(',') if l in TARGET_SPECIES])\n",
    "  df['onehot'] = df['label'].apply(lambda x: pd.NA if len(x) == 0 else OneHotEncoding(x, TARGET_SPECIES))\n",
    "  df.dropna(inplace=True)\n",
    "  # df[TARGET_SPECIES] = pd.DataFrame(df['code'].tolist(), index=df.index)  # Trans 'code' to specific species\n",
    "  df = df[['file', 'start time', 'end time', 'onehot']]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoDF = filterTarget(pd.read_csv(Path.cwd().parent.joinpath('data', f'auto-dataset.csv'), header=0))\n",
    "selfDF = filterTarget(pd.read_csv(Path.cwd().parent.joinpath('data', f'manual-dataset.csv'), header=0))\n",
    "openDF = filterTarget(pd.read_csv(Path.cwd().parent.joinpath('data', f'opensource-dataset.csv'), header=0))\n",
    "\n",
    "# Split <train, validate, test> = <0.8 : 0.1 : 0.1>\n",
    "# np.split |------------|--------------|----------|\n",
    "#          0   train   0.8  validate  0.9  test  1.0\n",
    "#          |     8      :      1       :     1    |\n",
    "# 8 : 1 : 1 -> cut point 0.8 * total length, and 0.9 * total length\n",
    "\n",
    "autoTrainDF, autoValidateDF, autoTestDF = np.split(autoDF.sample(frac=1), \n",
    "  [int(classifierRatio[0] * len(autoDF)), int((classifierRatio[0] + classifierRatio[1]) * len(autoDF))]\n",
    ")\n",
    "selfTrainDF, selfValidateDF, selfTestDF = np.split(selfDF.sample(frac=1), \n",
    "  [int(classifierRatio[0] * len(selfDF)), int((classifierRatio[0] + classifierRatio[1]) * len(selfDF))]\n",
    ")\n",
    "aecTrainDF = pd.concat([autoTrainDF, selfTrainDF, openDF], ignore_index=True)\n",
    "aecValidateDF = pd.concat([autoValidateDF, selfValidateDF], ignore_index=True)\n",
    "aecTestDF = pd.concat([autoTestDF, selfTestDF], ignore_index=True)\n",
    "aecTrainDF.to_csv(Path.cwd().parent.joinpath('data', 'tmp', f'aec-train.csv'), header=True, index=False)\n",
    "aecValidateDF.to_csv(Path.cwd().parent.joinpath('data', 'tmp', f'aec-validate.csv'), header=True, index=False)\n",
    "aecTestDF.to_csv(Path.cwd().parent.joinpath('data', 'tmp', f'aec-test.csv'), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeDF = pd.read_csv(Path.cwd().parent.joinpath('data', f'ae-dataset.csv'), header=0)\n",
    "aeTrainDF, aeValidateDF = np.split(aeDF.sample(frac=1), [int(autoencoderRatio[0] * len(aeDF))])\n",
    "aeTrainDF.to_csv(Path.cwd().parent.joinpath('data', 'tmp', f'ae-train.csv'), header=True, index=False)\n",
    "aeValidateDF.to_csv(Path.cwd().parent.joinpath('data', 'tmp', f'ae-validate.csv'), header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "802908908370ccb42dc2a7ac32386db1a5400a9392b38929d32ff89e4b8f01c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
